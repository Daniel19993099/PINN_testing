{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: tensorflow.compat.v1\n",
      "Other supported backends: tensorflow, pytorch, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\backend\\tensorflow_compat_v1\\tensor.py:25: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/replica:0/task:0/device:CPU:0', '/replica:0/task:0/device:CPU:1')\n",
      "Compiling model...\n",
      "Building feed-forward neural network...\n",
      "WARNING:tensorflow:From c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\nn\\tensorflow_compat_v1\\fnn.py:116: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'build' took 0.242058 s\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\model.py:168: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DistributedVariable.handle is not available outside the replica context or a `tf.distribute.Strategy.update()` call.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m dde\u001b[38;5;241m.\u001b[39mModel(data, net)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Compile the model with specific optimizer settings\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\model.py:137\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, lr, loss, metrics, decay, loss_weights, external_trainable_variables)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_trainable_variables \u001b[38;5;241m=\u001b[39m external_trainable_variables\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow.compat.v1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_tensorflow_compat_v1(lr, loss_fn, decay)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_tensorflow(lr, loss_fn, decay)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\model.py:194\u001b[0m, in \u001b[0;36mModel._compile_tensorflow_compat_v1\u001b[1;34m(self, lr, loss_fn, decay)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_losses_train \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39moutputs, losses_train]\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_losses_test \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39moutputs, losses_test]\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    195\u001b[0m     total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_name, learning_rate\u001b[38;5;241m=\u001b[39mlr, decay\u001b[38;5;241m=\u001b[39mdecay\n\u001b[0;32m    196\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\deepxde\\optimizers\\tensorflow_compat_v1\\optimizers.py:72\u001b[0m, in \u001b[0;36mget\u001b[1;34m(loss, optimizer, learning_rate, decay)\u001b[0m\n\u001b[0;32m     62\u001b[0m     optim \u001b[38;5;241m=\u001b[39m hvd\u001b[38;5;241m.\u001b[39mDistributedOptimizer(\n\u001b[0;32m     63\u001b[0m         optim,\n\u001b[0;32m     64\u001b[0m         compression\u001b[38;5;241m=\u001b[39mhvd_opt_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m         ],\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(update_ops):\n\u001b[1;32m---> 72\u001b[0m     train_op \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mminimize(loss, global_step\u001b[38;5;241m=\u001b[39mglobal_step)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_op\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:473\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, var_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    430\u001b[0m              gate_gradients\u001b[38;5;241m=\u001b[39mGATE_OP, aggregation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    431\u001b[0m              colocate_gradients_with_ops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    432\u001b[0m              grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Add operations to minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m  This method simply combines calls `compute_gradients()` and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(\n\u001b[0;32m    474\u001b[0m       loss, var_list\u001b[38;5;241m=\u001b[39mvar_list, gate_gradients\u001b[38;5;241m=\u001b[39mgate_gradients,\n\u001b[0;32m    475\u001b[0m       aggregation_method\u001b[38;5;241m=\u001b[39maggregation_method,\n\u001b[0;32m    476\u001b[0m       colocate_gradients_with_ops\u001b[38;5;241m=\u001b[39mcolocate_gradients_with_ops,\n\u001b[0;32m    477\u001b[0m       grad_loss\u001b[38;5;241m=\u001b[39mgrad_loss)\n\u001b[0;32m    479\u001b[0m   vars_with_grad \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    480\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vars_with_grad:\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:599\u001b[0m, in \u001b[0;36mOptimizer.compute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo variables to optimize.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    598\u001b[0m var_refs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mtarget() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m processors]\n\u001b[1;32m--> 599\u001b[0m grads \u001b[38;5;241m=\u001b[39m gradients\u001b[38;5;241m.\u001b[39mgradients(\n\u001b[0;32m    600\u001b[0m     loss, var_refs, grad_ys\u001b[38;5;241m=\u001b[39mgrad_loss,\n\u001b[0;32m    601\u001b[0m     gate_gradients\u001b[38;5;241m=\u001b[39m(gate_gradients \u001b[38;5;241m==\u001b[39m Optimizer\u001b[38;5;241m.\u001b[39mGATE_OP),\n\u001b[0;32m    602\u001b[0m     aggregation_method\u001b[38;5;241m=\u001b[39maggregation_method,\n\u001b[0;32m    603\u001b[0m     colocate_gradients_with_ops\u001b[38;5;241m=\u001b[39mcolocate_gradients_with_ops)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gate_gradients \u001b[38;5;241m==\u001b[39m Optimizer\u001b[38;5;241m.\u001b[39mGATE_GRAPH:\n\u001b[0;32m    605\u001b[0m   grads \u001b[38;5;241m=\u001b[39m control_flow_ops\u001b[38;5;241m.\u001b[39mtuple(grads)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:177\u001b[0m, in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Creating the gradient graph for control flow mutates Operations.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# _mutation_lock ensures a Session.run call cannot occur between creating and\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# mutating new ops.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m--> 177\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gradients_util\u001b[38;5;241m.\u001b[39m_GradientsHelper(\n\u001b[0;32m    178\u001b[0m       ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m    179\u001b[0m       gate_gradients, aggregation_method, stop_gradients,\n\u001b[0;32m    180\u001b[0m       unconnected_gradients)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:521\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.gradients is not supported when eager execution \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis enabled. Use tf.GradientTape instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m ys \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(_AsList(ys))\n\u001b[1;32m--> 521\u001b[0m xs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    522\u001b[0m     x\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;28;01mif\u001b[39;00m resource_variable_ops\u001b[38;5;241m.\u001b[39mis_resource_variable(x) \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m _AsList(xs)\n\u001b[0;32m    524\u001b[0m ]\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_ys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m   grad_ys \u001b[38;5;241m=\u001b[39m _AsList(grad_ys)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:522\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.gradients is not supported when eager execution \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis enabled. Use tf.GradientTape instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m ys \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(_AsList(ys))\n\u001b[0;32m    521\u001b[0m xs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 522\u001b[0m     x\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;28;01mif\u001b[39;00m resource_variable_ops\u001b[38;5;241m.\u001b[39mis_resource_variable(x) \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m _AsList(xs)\n\u001b[0;32m    524\u001b[0m ]\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_ys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m   grad_ys \u001b[38;5;241m=\u001b[39m _AsList(grad_ys)\n",
      "File \u001b[1;32mc:\\Users\\yesda\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:711\u001b[0m, in \u001b[0;36mDistributedVariable.handle\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    709\u001b[0m replica_id \u001b[38;5;241m=\u001b[39m values_util\u001b[38;5;241m.\u001b[39mget_current_replica_id_as_int()\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replica_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 711\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributedVariable.handle is not available outside the replica \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext or a `tf.distribute.Strategy.update()` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_packed_variable():\n",
      "\u001b[1;31mValueError\u001b[0m: DistributedVariable.handle is not available outside the replica context or a `tf.distribute.Strategy.update()` call."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import deepxde as dde\n",
    "import numpy as np\n",
    "\n",
    "# Set TensorFlow to use CPU only\n",
    "\n",
    "\n",
    "os.environ['DDE_BACKEND'] = 'tensorflow'\n",
    "# Define the spatial and time domains\n",
    "geom = dde.geometry.Interval(0, 1)\n",
    "time_domain = dde.geometry.TimeDomain(0, 1)\n",
    "geomtime = dde.geometry.GeometryXTime(geom, time_domain)\n",
    "\n",
    "# Define the PDE function\n",
    "def pde(x, y):\n",
    "    u = y\n",
    "    u_t = dde.grad.jacobian(y, x, i=0, j=1)\n",
    "    u_xx = dde.grad.hessian(y, x, i=0, j=0)\n",
    "    return u_t - u_xx\n",
    "\n",
    "# Define the boundary condition function\n",
    "def boundary_func(x):\n",
    "    return 0\n",
    "\n",
    "# Define the initial condition function\n",
    "def initial_func(x):\n",
    "    return np.sin(np.pi * x[:, 0:1])\n",
    "\n",
    "# Set up the boundary condition\n",
    "bc = dde.icbc.DirichletBC(geomtime, boundary_func, lambda _, on_boundary: on_boundary)\n",
    "\n",
    "# Set up the initial condition\n",
    "ic = dde.icbc.IC(geomtime, initial_func, lambda _, on_initial: on_initial)\n",
    "\n",
    "# Define observation data (for supervised learning)\n",
    "observe_x = np.vstack((np.linspace(0, 0.05, num=10), np.full((10), 1))).T\n",
    "observe_y = dde.icbc.PointSetBC(observe_x, np.sin(np.pi * observe_x[:, 0:1]))\n",
    "\n",
    "# Set up the data object\n",
    "data = dde.data.TimePDE(geomtime, pde, [ic, bc, observe_y], num_domain=40, num_test=10000)\n",
    "\n",
    "# Multi-CPU configuration\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/cpu:0\", \"/cpu:1\"])  # Specify CPU devices\n",
    "\n",
    "with strategy.scope():\n",
    "    # Set up the model and compile\n",
    "    net = dde.maps.FNN([2] + [32] * 3 + [1], \"tanh\", \"Glorot uniform\")\n",
    "    model = dde.Model(data, net)\n",
    "    \n",
    "    # Compile the model with specific optimizer settings\n",
    "    model.compile(\"adam\", lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    losshistory, train_state = model.train(epochs=10000)\n",
    "\n",
    "    # Save the results\n",
    "    dde.saveplot(losshistory, train_state, issave=True, isplot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
